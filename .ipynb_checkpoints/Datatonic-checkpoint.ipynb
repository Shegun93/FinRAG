{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a95734-b590-4bee-b9f1-a45c96553550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. Setup & Imports\n",
    "# ====================\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089cf12-1732-4f8e-b876-d0b54cf9f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 2. Load & Preprocess Data\n",
    "# ====================\n",
    "def load_financial():\n",
    "    df = pd.read_csv(\"finaicial.csv\")\n",
    "    return df\n",
    "df = load_financial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7c019-35fa-47b8-964b-a1825ff4d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38323bf-c65e-4637-acac-154485e53d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 4. Sentence Chunking\n",
    "# ====================\n",
    "def split_into_chunks(text, chunk_size=10):\n",
    "    \"\"\"Split text into chunks of `chunk_size` sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent) for sent in doc.sents]\n",
    "    return [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "# Process text and tables\n",
    "df[\"sentence_chunks\"] = df[\"text\"].apply(split_into_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77faef4-ae9f-4244-ad94-45ba4bf71563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5. Tokens Counts\n",
    "# =====================\n",
    "#token limits for all-mpnet-base-2 is 384\n",
    "def counts(text):\n",
    "    \"\"\"checking the tokens to see if its within the token limit\"\"\"\n",
    "    return len(text)\n",
    "df[\"sentence_token_counts\"] = df[\"sentence_chunks\"].apply(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b4cae-a10c-451a-bcb5-345910c64dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8794fb-3670-43b1-962c-fa1c0ae37111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a1189-8882-4117-bcf5-baa408bbe6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 6. Embedding Generation\n",
    "# ====================\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"Generate emdeddings layers\"\"\"\n",
    "    # Flatten chunks if they're nested lists\n",
    "    flat_chunks = [\" \".join(chunk) if isinstance(chunk, list) else chunk for chunk in chunks]\n",
    "    return embedding_model.encode(flat_chunks, convert_to_numpy=True)\n",
    "\n",
    "# Generate embeddings correctly\n",
    "df[\"text_embeddings\"] = df[\"sentence_chunks\"].apply(\n",
    "    lambda chunks: generate_embeddings(chunks)\n",
    ")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"Sample embeddings shape: {df['text_embeddings'].iloc[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b974ef5-5468-4a11-bcb6-761661596863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten chunks and embeddings\n",
    "all_text_chunks = [chunk for doc_chunks in df[\"sentence_chunks\"] for chunk in doc_chunks]\n",
    "all_text_embeddings = [emb for doc_embs in df[\"text_embeddings\"] for emb in doc_embs]\n",
    "\n",
    "# Check consistency\n",
    "assert len(all_text_chunks) == len(all_text_embeddings), \"Mismatch between chunks and embeddings!\"\n",
    "print(f\"Total chunks: {len(all_text_chunks)}, Total embeddings: {len(all_text_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc23f1a-7cfe-4eac-ac43-f2a716e89eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 7. Vector Search \n",
    "# ====================\n",
    "class VectorSearch:\n",
    "    def __init__(self, embeddings, texts):\n",
    "        # Stack embeddings into (N, 768) tensor\n",
    "        self.embeddings = torch.tensor(np.stack(embeddings), dtype=torch.float32).to(device)\n",
    "        self.texts = texts\n",
    "    \n",
    "    def search(self, query, top_k=1):\n",
    "        \n",
    "        \"\"\"Search for top_k most similar chunks (now defaults to top 1)\"\"\"\n",
    "        query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(query_embedding, self.embeddings)[0]\n",
    "        top_indices = torch.topk(cos_scores, k=top_k).indices.cpu().numpy()\n",
    "        return [(self.texts[i], cos_scores[i].item()) for i in top_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eea57-caa8-4fb6-9b60-8a6d45ee4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize search\n",
    "text_searcher = VectorSearch(all_text_embeddings, all_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbac919-3253-4217-9c05-7fd7a7c53c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 8. Example Query\n",
    "# ====================\n",
    "query = \"what was the increase in the operating profit for space systems from 2011 to 2012?\"\n",
    "results = text_searcher.search(query)\n",
    "\n",
    "print(f\"Top results for '{query}':\")\n",
    "for i, (chunk, score) in enumerate(results):\n",
    "    print(f\"\\nRank {i + 1} (Score: {score:.4f}):\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4e1b4-e838-4949-9391-8022ff6ba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Save in vector database\n",
    "# ========================\n",
    "pc = pinecone.Pinecone(api_key=\"pcsk_7B7VXN_6M4qLKUbxBrU4iCXs5VVy4ZCQCoTJUNJayD2EJa6PeqGygBfxzBb64YL2D56C9U\")\n",
    "index_name = \"datatonic-rags\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768, \n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\" \n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Index '{index_name}' is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf917479-f6b0-421a-9100-99b0b03c4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# for i in range(0, len(all_text_chunks), batch_size):\n",
    "#     batch_ids = [str(j) for j in range(i, min(i+batch_size, len(all_text_chunks)))]\n",
    "#     batch_vectors = [emb.tolist() for emb in all_text_embeddings[i:i+batch_size]]\n",
    "#     batch_metadata = [{\"text\": text} for text in all_text_chunks[i:i+batch_size]]\n",
    "    \n",
    "#     index.upsert(\n",
    "#         vectors=zip(batch_ids, batch_vectors, batch_metadata)\n",
    "#     )\n",
    "# print(\"Upsert Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517501a-1037-4e76-ae11-bb4102902dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_to_upsert = [\n",
    "    (\n",
    "        f\"vec_{i}\",  # Unique ID for each vector\n",
    "        emb.tolist() if hasattr(emb, 'tolist') else emb,  # Ensure it's a list\n",
    "        {\"text\": chunk}  # Store the text in metadata\n",
    "    )\n",
    "    for i, (chunk, emb) in enumerate(zip(all_text_chunks, all_text_embeddings))\n",
    "]\n",
    "\n",
    "# 2. Batch upsert (Pinecone recommends batches of 100-200)\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(vectors_to_upsert), batch_size)):\n",
    "    # Get batch of vectors\n",
    "    i_end = min(i+batch_size, len(vectors_to_upsert))\n",
    "    batch = vectors_to_upsert[i:i_end]\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    try:\n",
    "        index.upsert(vectors=batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting batch {i}-{i_end}: {e}\")\n",
    "        # Optionally: retry or save failed batches\n",
    "\n",
    "print(\"Upsert complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b11572-da47-48a1-9463-15383bebc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.describe_index_stats())\n",
    "sample_query = [0.1] * 768 \n",
    "results = index.query(vector=sample_query, top_k=1, include_metadata=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadca7d5-d2e3-439f-a19f-71e0dfe0c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query: str, top_k: int = 1):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    chunks = [match.metadata[\"text\"] for match in results.matches]\n",
    "    return chunks\n",
    "user_query = \"what was the increase in the operating profit for space systems from 2011 to 2012?\"\n",
    "relevant_chunks = query_pinecone(user_query)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9c44b-4fee-442b-8348-b11176ff2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 8. Save/Load System\n",
    "# ====================\n",
    "# # Save embeddings and metadata\n",
    "# pd.DataFrame({\n",
    "#     \"text\": all_text_chunks,\n",
    "#     \"embedding\": [emb.tolist() for emb in all_text_embeddings]\n",
    "# }).to_parquet(\"financial_embeddings.parquet\")\n",
    "        #chunks = [match.metadata[\"text\"] for match in results.matches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df397b23-1ffa-4b2b-a73f-774b87565ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeRetriever:\n",
    "    def __init__(self, index_name=\"datatonic-rags\", embedding_model=None):\n",
    "        self.pc = Pinecone(api_key=\"pcsk_7B7VXN_6M4qLKUbxBrU4iCXs5VVy4ZCQCoTJUNJayD2EJa6PeqGygBfxzBb64YL2D56C9U\")\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.embedding_model = embedding_model\n",
    "    def query(self, query: str, top_k: int = 1):\n",
    "        query_embedding = self.embedding_model.encode(query).tolist()\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        chunks_with_scores = [(match.metadata[\"text\"], match.score) for match in results.matches]\n",
    "        return chunks_with_scores\n",
    "retrieval = PineconeRetriever(embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a9a36bc-004e-4102-9b97-44aedefc0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['| 13.0% ( 13.0 % )\\nbacklog at year-end | $ 18900          | $ 20500          | $ 18100         \\n\\n2014 compared to 2013 space systems 2019 net sales for 2014 increased $ 107 million , or 1% ( 1 % ) , compared to 2013 .\\n', 'the increase was primarily attributable to higher net sales of approximately $ 340 million for the orion program due to increased volume ( primarily the first unmanned test flight of the orion mpcv ) ; and about $ 145 million for commercial space transportation programs due to launch-related activities .\\n', 'the increases were offset by lower net sales of approximately $ 335 million for government satellite programs due to decreased volume ( primarily aehf , gps-iii and muos ) ; and about $ 45 million for various other programs due to decreased volume .\\nspace systems 2019 operating profit for 2014 was comparable to 2013 .\\n', 'operating profit decreased by approximately $ 20 million for government satellite programs due to lower volume ( primarily aehf and gps-iii ) , partially offset by increased risk retirements ( primarily muos ) ; and about $ 20 million due to decreased equity earnings for joint ventures .\\n', 'the decreases were offset by higher operating profit of approximately $ 30 million for the orion program due to increased volume .\\noperating profit was reduced by approximately $ 40 million for charges , net of recoveries , related to the restructuring action announced in november 2013 .\\n', 'adjustments not related to volume , including net profit booking rate adjustments and other matters , were approximately $ 10 million lower for 2014 compared to 2013 .\\n', '2013 compared to 2012 space systems 2019 net sales for 2013 decreased $ 389 million , or 5% ( 5 % ) , compared to 2012 .\\n', 'the decrease was primarily attributable to lower net sales of approximately $ 305 million for commercial satellite programs due to fewer deliveries ( zero delivered during 2013 compared to two for 2012 ) ; and about $ 290 million for the orion program due to lower volume .\\n', 'the decreases were partially offset by higher net sales of approximately $ 130 million for government satellite programs due to net increased volume ; and about $ 65 million for strategic and defensive missile programs ( primarily fbm ) due to increased volume and risk retirements .\\n', 'the increase for government satellite programs was primarily attributable to higher volume on aehf and other programs , partially offset by lower volume on goes-r , muos and sbirs programs .\\nspace systems 2019 operating profit for 2013 decreased $ 38 million , or 4% ( 4 % ) , compared to 2012 .\\n'], 0.748807371)]\n"
     ]
    }
   ],
   "source": [
    "user_query = \"what was the increase in the operating profit for space systems from 2011 to 2012?\"\n",
    "relevant_chunks = retrieval.query(user_query)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8da9ad9-4286-403e-a33f-a0d3cab26b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 12 GB\n",
      "GPU memory: 12 | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\n",
      "use_quantization_config set to: False\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "#=====================\n",
    "#10. llm loading\n",
    "#=====================\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22932b04-1172-4837-a0a6-765d8033ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================\n",
    "#10. llm setup\n",
    "#=====================\n",
    "# setup/libraries\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e64edab7-65f5-453e-8807-c38b0e944d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:43<00:00, 81.75s/it]\n"
     ]
    }
   ],
   "source": [
    "#=====================\n",
    "#11. llm loading\n",
    "#=====================\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "Gamma_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation) \n",
    "if not use_quantization_config:\n",
    "    Gamma_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23085aeb-5ede-44ad-9364-040b7ddd72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 11. Unified RAG Query Function (Updated)\n",
    "# ====================\n",
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True,\n",
    "        task=\"qa\"):\n",
    "    \n",
    "    # --- RETRIEVAL ---\n",
    "# In ask() function, change:\n",
    "    top_chunk, score = retrieval.query(query, top_k=1)[0]  # Get first result    \n",
    "    # --- PROMPT FORMATTING ---\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "    \n",
    "    Question: {query}\n",
    "    Context: {top_chunk if isinstance(top_chunk, str) else ' '.join(top_chunk)}\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # --- GENERATION ---\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(Gamma_model.device)\n",
    "    outputs = Gamma_model.generate(\n",
    "        **inputs,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # --- RESPONSE CLEANING ---\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response.replace(prompt, \"\").strip() if format_answer_text else full_response\n",
    "    \n",
    "    # --- RETURN ---\n",
    "    return answer if return_answer_only else (answer, top_chunk, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dc9e315-0bb6-4044-86a1-87030e1c2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# Testing Function (Updated)\n",
    "# ====================\n",
    "def test_rag_system(query: str, \n",
    "                   show_context: bool = True,\n",
    "                   max_new_tokens: int = 256) -> str:\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    answer, context_chunk, score = ask(\n",
    "        query=query,\n",
    "        return_answer_only=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        task=\"qa\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nANSWER:\\n{answer}\\n\")\n",
    "    \n",
    "    if show_context:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"CONTEXT (Score: {score:.4f}):\")\n",
    "        #print(textwrap.fill(str(context_chunk)[:500], width=80))\n",
    "        print(f\"{'-'*50}\")\n",
    "        # Add this to your test_rag_system() before generation:\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3dff0d6e-7c18-47f8-867f-f82329d368e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUERY: what is the total of estimated future contingent acquisition obligations payable in cash in 2009?\n",
      "==================================================\n",
      "\n",
      "ANSWER:\n",
      "$ 5.5  |  |  |  |  |  |  | 5.5   \n",
      " **Explanation:**  The total of estimated future contingent acquisition obligations payable in cash in 2009 is $5.5 million.\n",
      "\n",
      "--------------------------------------------------\n",
      "CONTEXT (Score: 0.7406):\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize searcher (do this once)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"what is the total of estimated future contingent acquisition obligations payable in cash in 2009?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_rag_system(query)\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6341c-8d61-4b3b-ac96-628b6410fc60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
