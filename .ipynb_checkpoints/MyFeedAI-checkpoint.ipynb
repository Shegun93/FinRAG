{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a24829-1596-40c1-8f9c-faa759c47c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ba480-254c-47a6-be60-ba6136081158",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding = pd.read_csv(\"embeddings_CNN.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding[\"embedding\"] = text_chunks_and_embedding[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f3946-29fd-43c4-a5cd-b3c4f1226d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd0afc-7fbf-406a-a7a6-ae9ba153eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding['sentence_chunk'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255c69a-341a-4670-9f7c-4daf519a7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already imported the model and embeddings are on the GPU\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "def vector_search(query, embeddings, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query], device=\"cuda\")\n",
    "    query_embedding_tensor = torch.tensor(query_embedding).to(\"cuda\")\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_embedding_tensor, embeddings)\n",
    "\n",
    "    # Get the top_k most similar results\n",
    "    top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    top_k_scores = similarities[top_k_indices]\n",
    "\n",
    "    # Return the top_k results with their indices and similarity scores\n",
    "    return [(top_k_indices[i].item(), top_k_scores[i].item()) for i in range(top_k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2717a57-6fe1-4e6d-a2d1-567f94dbf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already imported the model and embeddings are on the GPU\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "def vector_search(query, embeddings, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query], device=\"cuda\")\n",
    "    query_embedding_tensor = torch.tensor(query_embedding).to(\"cuda\")\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_embedding_tensor, embeddings)\n",
    "    # Get the top_k most similar results\n",
    "    top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    top_k_scores = similarities[top_k_indices]\n",
    "    # Return the top_k results with their indices and similarity scores\n",
    "    return [(top_k_indices[i].item(), top_k_scores[i].item()) for i in range(top_k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa4477-c24d-4742-a4f1-ad1a467f1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ce64d-65c0-4dbc-a06e-f55fdf2481d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_fn(query: str,\n",
    "               embeddings = torch.tensor,\n",
    "                model: SentenceTransformer = embedding_model,\n",
    "                n_resources_to_return: int = 5):\n",
    "    query_embedding = model.encode(query,\n",
    "                                  convert_to_tensor=True\n",
    "                                  )\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                 k = n_resources_to_return)\n",
    "    return scores, indices\n",
    "def print_top_score(query: str,\n",
    "                    embeddings: torch.tensor,\n",
    "                    pages_and_chunks: list[dict] = pages_and_chunks,\n",
    "                    n_resources_to_return: int = 1) -> str:\n",
    "\n",
    "    # Retrieve scores and indices (limit to 1 resource to return)\n",
    "    scores, indices = retrieve_fn(query=query,\n",
    "                                  embeddings=embeddings,\n",
    "                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    # Extract the top text chunk based on the first index\n",
    "    top_chunk = pages_and_chunks[int(indices[0])][\"sentence_chunk\"]\n",
    "    \n",
    "    # Print the retrieved top chunk\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    # Return the topmost chunk\n",
    "    return top_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153210b6-c6ac-430e-a161-e491e6c1d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"what happened in Capital hall\"\n",
    "scores, indices = retrieve_fn(query=query_1,\n",
    "                                    embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e679028-cd0b-4d14-94d1-deca012e6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_score(query=query_1,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabbeee-6625-41ce-bf91-5745f77b3e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84d0a4-88e3-4934-bf11-db406f56d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3f4ea-e0d2-4405-bbbb-0bf455eb81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "Gamma_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation) \n",
    "if not use_quantization_config:\n",
    "    Gamma_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea2ce0-6325-4644-9487-9d4bf8ae6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fb54f-72a8-4e26-9f24-16fd9a17486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Formats a query with context items and styles the prompt for Gamma 2B.\n",
    "    \"\"\"\n",
    "    # Combine context items into a single readable string\n",
    "    context = \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "    \n",
    "    # Base prompt with examples of the desired style and tone\n",
    "    base_prompt = f\"\"\"Based on the following context items, answer the query in a tone that resonates with Gen Z.\n",
    "Keep it fresh, conversational, and culturally relevant. Use memes, humor, or slang where it fits, but stay accurate.\n",
    "\n",
    "Context:\n",
    "- {context}\n",
    "    \n",
    "Examples for the tone:\n",
    "Example 1:\n",
    "User: What’s going on with the latest climate change report?\n",
    "Gamma 2B: Okay, so here’s the deal: the new climate report is throwing major shade at humanity. TL;DR—things are heating up, literally. Greenhouse gases? Still through the roof. Polar ice caps? Melting faster than ice cream in July. The report’s basically yelling, “Do something!” So, yeah, less single-use plastics and more renewable energy, please. 🌍  \n",
    "\n",
    "Example 2:\n",
    "User: What’s the latest update on the election results?\n",
    "Gamma 2B: Alright, election updates are in, and it’s a rollercoaster. The votes are being counted like it’s a TikTok trend—slow but steady. Key states are swinging, but it’s still anyone’s game. Keep your snacks close and your notifications on, ‘cause it’s about to get spicy. 🗳️  \n",
    "\n",
    "Example 3:\n",
    "User: Why is everyone talking about the new tech layoffs?\n",
    "Gamma 2B: Oof, big yikes for the tech world right now. Companies like Meta and Google have been downsizing—like, thousands of jobs gone in a blink. They’re blaming “economic uncertainty,” but let’s be real, it’s probs also about squeezing more profit. If you’re in tech, it’s giving *stay-ready-so-you-don’t-have-to-get-ready* vibes.  \n",
    "\n",
    "Now, answer the query below:\n",
    "User: {query}\n",
    "Gamma 2B:\"\"\"\n",
    "    \n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09ae2d-84ad-4338-9213-1e6f8cee19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Generates an answer to a query by retrieving context items, formatting the prompt, \n",
    "    and calling Gamma 2B to generate a response.\n",
    "    \"\"\"\n",
    "    # Retrieve context items\n",
    "    scores, indices = retrieve_fn(query=query, embeddings=embeddings)\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Attach scores to context items\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu()  # Return score to CPU\n",
    "    \n",
    "    # Format the prompt using Gamma 2B requirements\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items)\n",
    "    \n",
    "    # Prepare the input for Gamma 2B\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = Gamma_model.generate(\n",
    "        **input_ids,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Format the answer text if requested\n",
    "    if format_answer_text:\n",
    "        output_text = (\n",
    "            output_text\n",
    "            .replace(prompt, \"\")\n",
    "            .strip()\n",
    "        )\n",
    "    \n",
    "    # Return only the answer or both the answer and context\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6574e57-7fbe-42aa-968a-efa443fc037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Nigeria's Capital?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer the query with context and return context items\n",
    "answer, context_items = ask(\n",
    "    query=query, \n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    return_answer_only=False\n",
    ")\n",
    "\n",
    "# Print the answer\n",
    "print(f\"Answer:\\n{answer}\")\n",
    "for item in context_items:\n",
    "    print(f\"- {item['sentence_chunk']} (Score: {item['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9c6e5-bcb8-417c-8dc0-754e74249882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Initialize the TTS models and processor globally\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "tts_model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(\"cuda\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a5c5e-6254-4e6c-8aa1-ad76c2b73165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=300):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of max_length characters without cutting sentences.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)  # Split text by sentence-ending punctuation\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b1047-dfd5-4599-ba18-5d629f746461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, speaker_embedding, output_path):\n",
    "    \"\"\"\n",
    "    Convert text to speech using Microsoft SpeechT5, handle long text, and smooth audio transitions.\n",
    "    \"\"\"\n",
    "    max_chunk_length = 300  # Limit for chunking text\n",
    "    chunks = split_text(text, max_length=max_chunk_length)\n",
    "    audio_chunks = []\n",
    "\n",
    "    print(f\"Splitting text into {len(chunks)} chunks for processing...\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i + 1}/{len(chunks)}: {chunk}\")\n",
    "\n",
    "        # Preprocess the input chunk\n",
    "        inputs = processor(text=chunk, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate the speech audio\n",
    "        with torch.no_grad():\n",
    "            speech = tts_model.generate_speech(\n",
    "                inputs[\"input_ids\"], \n",
    "                speaker_embeddings=speaker_embedding, \n",
    "                vocoder=vocoder\n",
    "            )\n",
    "\n",
    "        # Convert the speech tensor to a numpy array\n",
    "        speech_numpy = speech.cpu().numpy()\n",
    "        audio_chunks.append(speech_numpy)\n",
    "\n",
    "    # Concatenate all audio chunks\n",
    "    final_audio = np.concatenate(audio_chunks)\n",
    "\n",
    "    # Save the concatenated audio as a .wav file\n",
    "    write(output_path, 16000, final_audio)  # 16kHz sample rate\n",
    "    print(f\"Final audio saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a109b4c-f2df-4ef5-a4b5-fda830af79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    \"\"\"\n",
    "    Query Gamma 2B to generate a response and retrieve the top-1 context.\n",
    "    \"\"\"\n",
    "    # Generate response and retrieve context items\n",
    "    print(\"Generating response from Gamma 2B...\")\n",
    "    answer, context_items = ask(query=query, return_answer_only=False)\n",
    "\n",
    "    # Extract the top-1 context\n",
    "    top_context = context_items[0] if context_items else {\"sentence_chunk\": \"No context available.\"}\n",
    "\n",
    "    # Print the answer and top context\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(answer)\n",
    "    print(\"\\nTop-1 Context:\")\n",
    "    print(f\"- {top_context['sentence_chunk']} (Score: {top_context.get('score', 0):.2f})\")\n",
    "\n",
    "    # Return the response and top-1 context\n",
    "    return answer, top_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34482e53-b3f0-4778-aa9b-cd852360ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop():\n",
    "    \"\"\"\n",
    "    Main loop for querying Gamma 2B and converting its output to speech.\n",
    "    Only the Gamma model's output is passed to TTS, excluding the context.\n",
    "    \"\"\"\n",
    "    output_folder = \"/home/shegun93/TTS\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    while True:\n",
    "        # Ask the user for a query\n",
    "        query_text = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query_text.lower() == 'exit':\n",
    "            print(\"Exiting the program. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Generate the response from Gamma 2B\n",
    "        answer, _ = generate_response(query=query_text)  # Ignore the context\n",
    "\n",
    "        # Generate a filename for the audio file\n",
    "        audio_filename = os.path.join(output_folder, \"gamma_response_1.wav\")\n",
    "\n",
    "        # Convert the answer to speech and save the audio\n",
    "        print(\"\\nSaving the Gamma model's output as speech:\")\n",
    "        text_to_speech(answer, speaker_embedding, audio_filename)\n",
    "if __name__ == \"__main__\":\n",
    "    main_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
