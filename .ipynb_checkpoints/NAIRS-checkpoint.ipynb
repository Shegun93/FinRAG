{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4925f405-61e9-4c99-9d00-766f2225208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "nlp = English()\n",
    "import re\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bce6c-8061-4602-8952-b1705fa11a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"Maths.pdf\"\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() \n",
    "    return cleaned_text\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  \n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({ \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed11a8-b32e-4ea8-8f92-cbd0b6afe35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_and_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833eb54c-40d0-41fc-8f3e-7c8361df2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa303ecf-9051-4267-ae64-f2891566efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"sentencizer\")\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9b3e0-db44-4c69-acac-09ac25127fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b9786-e7ab-4b49-9dfb-e39cdf38fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1acd08-4a44-4e06-bbde-7ad7729f515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bfdbf-1ac2-42ba-b731-2b9bf97422cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_length = 318\n",
    "for row in df[df[\"chunk_token_count\"] >= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9d8d1-02d8-4d08-850b-fca2f2953474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16f679-188f-4cec-8279-a8216c356f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_token_length = 318\n",
    "for row in df[df[\"chunk_token_count\"] >= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a26d7-43ae-4342-9991-c94cd1d2d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_max_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_max_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacab186-fc78-4d93-a02d-86a6ccf1aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_and_chunks_over_max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4abdf4-c78e-4381-9c13-c7e8b45ac649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(pages_and_chunks_over_max_token_len)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75177a8-f2cc-488a-a151-38afcde973a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['sentence_chunk'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454074eb-2549-4f2c-ab94-10d860e8cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f1c06-95fb-4090-be06-057061400316",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdff3a3-932d-42ec-94f3-897b14c05e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.0)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for item in tqdm(pages_and_chunks):\\n    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "\"\"\"for item in tqdm(pages_and_chunks):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cd783-a9e2-42e9-aa3a-622ceea87183",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks)\n",
    "embeddings_df_save_path = \"embeddings_df_2.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16084e71-0858-4b17-bd24-3b94e14a4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = pd.read_csv('embeddings_df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b46d002-67a8-41e4-850e-e73e1be07297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11188, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = \"cuda\"\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding = pd.read_csv(\"embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding[\"embedding\"] = text_chunks_and_embedding[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd35e28-e041-409d-b802-a48cac01f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already imported the model and embeddings are on the GPU\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "def vector_search(query, embeddings, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query], device=\"cuda\")\n",
    "    query_embedding_tensor = torch.tensor(query_embedding).to(\"cuda\")\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_embedding_tensor, embeddings)\n",
    "\n",
    "    # Get the top_k most similar results\n",
    "    top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    top_k_scores = similarities[top_k_indices]\n",
    "\n",
    "    # Return the top_k results with their indices and similarity scores\n",
    "    return [(top_k_indices[i].item(), top_k_scores[i].item()) for i in range(top_k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f21f3-9bf3-4476-aafe-93b014434fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec17b95-b8d2-4d91-a95d-b246b0692321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "def retrieve_fn(query: str,\n",
    "               embeddings = torch.tensor,\n",
    "                model: SentenceTransformer = embedding_model,\n",
    "                n_resources_to_return: int = 5):\n",
    "    query_embedding = model.encode(query,\n",
    "                                  convert_to_tensor=True\n",
    "                                  )\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                 k = n_resources_to_return)\n",
    "    return scores, indices\n",
    "def print_top_score(query: str,\n",
    "                    embeddings: torch.tensor,\n",
    "                    pages_and_chunks: list[dict] = pages_and_chunks,\n",
    "                    n_resources_to_return: int = 1) -> str:\n",
    "\n",
    "    # Retrieve scores and indices (limit to 1 resource to return)\n",
    "    scores, indices = retrieve_fn(query=query,\n",
    "                                  embeddings=embeddings,\n",
    "                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    # Extract the top text chunk based on the first index\n",
    "    top_chunk = pages_and_chunks[int(indices[0])][\"sentence_chunk\"]\n",
    "    \n",
    "    # Print the retrieved top chunk\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    # Return the topmost chunk\n",
    "    return top_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90cca1f-581f-42a3-8830-c8a92361c467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5181, 0.5044, 0.5020, 0.4877, 0.4822], device='cuda:0'),\n",
       " tensor([4781, 4977, 2460, 4988, 9322], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1 = \"Solve the quadratic equation: 3x^8 + 5x - 2 = 0.\"\n",
    "scores, indices = retrieve_fn(query=query_1,\n",
    "                                    embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c7fc04-e31e-4749-892a-449601a7a5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Solve the quadratic equation: 3x^8 + 5x - 2 = 0.'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Solving this equation for x,  we obtain x = ±3√. Next method is 2. By Completing the Square: This is a method that can be used to solve any quadratic equation. First note that  X2+bx+(b/2)2=(x+b2)2.                        (1) Example. Solve the equation x2−6x−10=0 by completing the square. Solution. By adding 10 to each side of the equation, we obtain X2−6x=10.                                 (2) Note that half of the coefficient of x is −6/2=−3. Add (−3)2 to each side of (2):'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_top_score(query=query_1,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eafbda-caa9-4a50-8716-34c4e9914092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097a6f0-58ac-4201-87e4-8f6acfb4c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "Gamma_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation) \n",
    "if not use_quantization_config:\n",
    "    Gamma_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ff221-6120-4bc7-af97-8cc3b2892570",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881642b0-0814-4cc7-899d-625e7649fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict], task: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats a query with context items and styles the prompt for professional tone.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The student's query or topic.\n",
    "        context_items (list[dict]): A list of context items with key \"sentence_chunk\".\n",
    "        task (str): The task to perform - \"questions\", \"evaluation\", or \"explanation\".\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted prompt for the task.\n",
    "    \"\"\"\n",
    "    # Combine context items into a single readable string\n",
    "    context = \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "    \n",
    "    # Base prompt for generating assessment questions\n",
    "    if task == \"questions\":\n",
    "        base_prompt = f\"\"\"Based on the following context items, generate three assessment questions to evaluate the student's understanding of the topic:\n",
    "1. A basic question to test foundational knowledge.\n",
    "2. An application-based question to assess problem-solving ability.\n",
    "3. A conceptual question to challenge deeper understanding.\n",
    "\n",
    "Context:\n",
    "- {context}\n",
    "\n",
    "Examples:\n",
    "Example 1:\n",
    "Context: Newton's second law states that Force equals mass times acceleration (F = ma).\n",
    "Generated Questions:\n",
    "1. What does Newton's second law state?\n",
    "2. If a 10 kg object is pushed with a force of 20 N, what is its acceleration?\n",
    "3. Why is Newton’s second law important in understanding motion?\n",
    "\n",
    "Now, generate questions for the topic based on the context above:\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "    # Base prompt for evaluating student answers\n",
    "    elif task == \"evaluation\":\n",
    "        base_prompt = f\"\"\"Evaluate the following student answers based on the provided context and expected answers:\n",
    "1. Mark the answer as Correct, Partially Correct, or Incorrect.\n",
    "2. Provide a detailed explanation of why the answer is correct or incorrect.\n",
    "\n",
    "Context:\n",
    "- {context}\n",
    "\n",
    "Questions and Student Answers:\n",
    "{query}\n",
    "\n",
    "Expected Answers:\n",
    "Provide detailed feedback for each question and answer:\n",
    "\"\"\"\n",
    "    # Base prompt for providing tailored explanations\n",
    "    elif task == \"explanation\":\n",
    "        base_prompt = f\"\"\"The student struggled with the following concepts. Provide a clear and concise explanation for each, including examples where appropriate.\n",
    "\n",
    "Context:\n",
    "- {context}\n",
    "\n",
    "Misunderstood Concepts:\n",
    "{query}\n",
    "\n",
    "Examples:\n",
    "Example 1:\n",
    "Concept: F = ma means Force = mass times acceleration.\n",
    "Explanation: Newton's second law explains how force, mass, and acceleration are related. For example, if you apply a force of 20 N to a 5 kg object, the acceleration is 4 m/s² because F = ma.\n",
    "\n",
    "Provide tailored explanations for the concepts based on the context:\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Use 'questions', 'evaluation', or 'explanation'.\")\n",
    "\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51f9c1-dfdb-412d-a844-f0dc4ede1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict], task: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats a query with context items and styles the prompt for professional tone and conversational clarity.\n",
    "    \"\"\"\n",
    "    # Combine context items\n",
    "    context = \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    if task == \"solve_and_explain\":\n",
    "        base_prompt = f\"\"\"You are a helpful assistant who solves problems step-by-step and explains them clearly.\n",
    "\n",
    "Context:\n",
    "- {context}\n",
    "\n",
    "Let's solve this step-by-step:\n",
    "\n",
    "1. **Understand the Problem:** Analyze the given equation to identify its components.\n",
    "2. **Solve Step-by-Step:** Show each calculation in detail.\n",
    "3. **Explain the Solution:** Summarize the results and explain the reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem:\n",
    "{query}\n",
    "\n",
    "### Solution:\n",
    "Step 1: Identify the coefficients in the quadratic equation.\n",
    "Step 2: Apply the quadratic formula.\n",
    "Step 3: Perform the calculations.\n",
    "Step 4: Interpret the results.\n",
    "\n",
    "---\n",
    "\n",
    "Provide the solution in the format above. Be clear and conversational in your explanation.\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Use 'solve_and_explain' for this template.\")\n",
    "\n",
    "    return base_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c9fc5-5208-450b-967b-4a2fb0b215e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, context_items: list[dict], task: str) -> str:\n",
    "    \"\"\"\n",
    "    Custom formatter to structure the prompt for generating questions, evaluation, explanation, or problem-solving.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user's query or topic.\n",
    "        context_items (list[dict]): A list of context items with key \"sentence_chunk\".\n",
    "        task (str): The task to perform - \"questions\", \"evaluation\", \"explanation\", or \"solve_and_explain\".\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt based on the specified task.\n",
    "    \"\"\"\n",
    "    # Combine context into a concise, unique format\n",
    "    unique_context = list({item[\"sentence_chunk\"] for item in context_items})  # Remove duplicates\n",
    "    context = \"\\n\".join([f\"- {chunk}\" for chunk in unique_context]) if unique_context else \"No specific context provided.\"\n",
    "\n",
    "    # Task-specific formatting\n",
    "    if task == \"questions\":\n",
    "        formatted_prompt = f\"\"\"\n",
    "Task: Generate assessment questions to test a student's understanding of the topic.\n",
    "\n",
    "Context (for reference only, do not repeat in your response):\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Create three questions:\n",
    "   - A foundational knowledge question (easy).\n",
    "   - An application-based question (moderate).\n",
    "   - A conceptual question to assess deeper understanding (challenging).\n",
    "2. Ensure the questions are clear, concise, and focused on the topic.\n",
    "\n",
    "Topic: {query}\n",
    "\n",
    "Now, generate the questions.\n",
    "\"\"\"\n",
    "    elif task == \"evaluation\":\n",
    "        formatted_prompt = f\"\"\"\n",
    "Task: Evaluate student responses to the given questions.\n",
    "\n",
    "Context (for reference only, do not repeat in your response):\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Review the provided student answers.\n",
    "2. For each answer, classify as Correct, Partially Correct, or Incorrect.\n",
    "3. Provide detailed feedback explaining why the answer is correct or incorrect.\n",
    "\n",
    "Questions and Student Answers:\n",
    "{query}\n",
    "\n",
    "Now, evaluate and provide feedback for each response.\n",
    "\"\"\"\n",
    "    elif task == \"explanation\":\n",
    "        formatted_prompt = f\"\"\"\n",
    "Task: Provide clear and concise explanations for the following concepts.\n",
    "\n",
    "Context (for reference only, do not repeat in your response):\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Write a clear explanation for each concept in the query.\n",
    "2. Include examples to enhance understanding where appropriate.\n",
    "3. Use simple and precise language.\n",
    "\n",
    "Concepts:\n",
    "{query}\n",
    "\n",
    "Now, provide the explanations.\n",
    "\"\"\"\n",
    "    elif task == \"solve_and_explain\":\n",
    "        formatted_prompt = f\"\"\"\n",
    "Task: Solve the following problem step-by-step and explain the solution.\n",
    "\n",
    "Context (for reference only, do not repeat in your response):\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Solve the problem in clear, logical steps.\n",
    "2. Explain each step as if teaching a beginner.\n",
    "3. Provide a detailed solution and examples to enhance understanding.\n",
    "\n",
    "Problem:\n",
    "{query}\n",
    "\n",
    "Now, solve the problem and explain the solution.\n",
    "\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Use 'questions', 'evaluation', 'explanation', or 'solve_and_explain'.\")\n",
    "\n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f74c57-c4d7-4338-9bc7-0d1137243a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True,\n",
    "        task=\"questions\"):  # Add a task argument to pass to prompt_formatter\n",
    "    \"\"\"\n",
    "    Generates an answer to a query by retrieving context items, formatting the prompt, \n",
    "    and calling Gamma 2B to generate a response.\n",
    "    \"\"\"\n",
    "    # Retrieve context items\n",
    "    scores, indices = retrieve_fn(query=query, embeddings=embeddings)\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Attach scores to context items\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu()  # Return score to CPU\n",
    "    \n",
    "    # Format the prompt using Gamma 2B requirements\n",
    "    prompt = prompt_formatter(query=query, context_items=context_items, task=task)  # Pass the task parameter\n",
    "    \n",
    "    # Prepare the input for Gamma 2B\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = Gamma_model.generate(\n",
    "        **input_ids,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Format the answer text if requested\n",
    "    if format_answer_text:\n",
    "        output_text = (\n",
    "            output_text\n",
    "            .replace(prompt, \"\")\n",
    "            .strip()\n",
    "        )\n",
    "    \n",
    "    # Return only the answer or both the answer and context\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    return output_text, context_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5bf40-4790-4a38-957b-f05a43e785eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Solve the quadratic equation: 3x^2 + 5x - 2 = 0.\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer the query with context and return context items\n",
    "answer, context_items = ask(\n",
    "    query=query, \n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,\n",
    "    return_answer_only=False,\n",
    "    task=\"solve_and_explain\"  # Specify the task explicitly\n",
    ")\n",
    "print(\"\\nContext Items:\")\n",
    "if isinstance(context_items, list):\n",
    "    for idx, item in enumerate(context_items, start=1):\n",
    "        if isinstance(item, dict):\n",
    "            chunk = item.get(\"sentence_chunk\", \"Unknown Chunk\")\n",
    "            score = item.get(\"score\", 0)\n",
    "            print(f\"{idx}. {chunk}\\n   (Score: {score:.2f})\\n\")\n",
    "        else:\n",
    "            print(f\"Unexpected context format: {item}\")\n",
    "else:\n",
    "    print(f\"Unexpected context_items format: {context_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f96e5-fc2c-4ba2-a99c-66446367602c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
