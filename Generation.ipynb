{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9181ec1-53c4-4884-8f99-20586a40f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/TTS/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. Setup & Imports\n",
    "# ====================\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from pinecone import Pinecone\n",
    "from tqdm.auto import tqdm\n",
    "load_dotenv()\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238a5966-d18e-4b92-a49e-2de024b36745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================\n",
    "#3. llm setup\n",
    "#=====================\n",
    "# setup/libraries\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afeca6d-f9e8-4f3c-8c74-e2c5527b0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeRetriever:\n",
    "    def __init__(self, index_name=\"datatonic-rags\", embedding_model=None):\n",
    "        vector_db = os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.pc = pinecone.Pinecone(api_key=vector_db)\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.embedding_model = embedding_model\n",
    "    def query(self, query: str, top_k: int = 1):\n",
    "        query_embedding = self.embedding_model.encode(query).tolist()\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        chunks_with_scores = [(match.metadata[\"text\"], match.score) for match in results.matches]\n",
    "        return chunks_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f54811-03a2-4b60-ad03-0ec513832252",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = PineconeRetriever(embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535f48be-da25-4500-9cac-7ce62d4b4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['| 13.0% ( 13.0 % )\\nbacklog at year-end | $ 18900          | $ 20500          | $ 18100         \\n\\n2014 compared to 2013 space systems 2019 net sales for 2014 increased $ 107 million , or 1% ( 1 % ) , compared to 2013 .\\n', 'the increase was primarily attributable to higher net sales of approximately $ 340 million for the orion program due to increased volume ( primarily the first unmanned test flight of the orion mpcv ) ; and about $ 145 million for commercial space transportation programs due to launch-related activities .\\n', 'the increases were offset by lower net sales of approximately $ 335 million for government satellite programs due to decreased volume ( primarily aehf , gps-iii and muos ) ; and about $ 45 million for various other programs due to decreased volume .\\nspace systems 2019 operating profit for 2014 was comparable to 2013 .\\n', 'operating profit decreased by approximately $ 20 million for government satellite programs due to lower volume ( primarily aehf and gps-iii ) , partially offset by increased risk retirements ( primarily muos ) ; and about $ 20 million due to decreased equity earnings for joint ventures .\\n', 'the decreases were offset by higher operating profit of approximately $ 30 million for the orion program due to increased volume .\\noperating profit was reduced by approximately $ 40 million for charges , net of recoveries , related to the restructuring action announced in november 2013 .\\n', 'adjustments not related to volume , including net profit booking rate adjustments and other matters , were approximately $ 10 million lower for 2014 compared to 2013 .\\n', '2013 compared to 2012 space systems 2019 net sales for 2013 decreased $ 389 million , or 5% ( 5 % ) , compared to 2012 .\\n', 'the decrease was primarily attributable to lower net sales of approximately $ 305 million for commercial satellite programs due to fewer deliveries ( zero delivered during 2013 compared to two for 2012 ) ; and about $ 290 million for the orion program due to lower volume .\\n', 'the decreases were partially offset by higher net sales of approximately $ 130 million for government satellite programs due to net increased volume ; and about $ 65 million for strategic and defensive missile programs ( primarily fbm ) due to increased volume and risk retirements .\\n', 'the increase for government satellite programs was primarily attributable to higher volume on aehf and other programs , partially offset by lower volume on goes-r , muos and sbirs programs .\\nspace systems 2019 operating profit for 2013 decreased $ 38 million , or 4% ( 4 % ) , compared to 2012 .\\n'], 0.748807371)]\n"
     ]
    }
   ],
   "source": [
    "user_query = \"what was the increase in the operating profit for space systems from 2011 to 2012?\"\n",
    "relevant_chunks = retrieval.query(user_query)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8357e694-3f1f-48a0-bc0d-6312b8b402a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:45<00:00, 52.90s/it]\n"
     ]
    }
   ],
   "source": [
    "#=====================\n",
    "#4. llm loading\n",
    "#=====================\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "use_quantization_config = False\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "Gamma_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False,\n",
    "                                                 attn_implementation=attn_implementation) \n",
    "if not use_quantization_config:\n",
    "    Gamma_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38c91d2-9906-406b-89a8-f6e029721ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 5. RAG Query\n",
    "# ====================\n",
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True,\n",
    "        task=\"qa\"):\n",
    "    \n",
    "    # --- RETRIEVAL ---\n",
    "# In ask() function, change:\n",
    "    top_chunk, score = retrieval.query(query, top_k=1)[0]  # Get first result    \n",
    "    # --- PROMPT FORMATTING ---\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "    \n",
    "    Question: {query}\n",
    "    Context: {top_chunk if isinstance(top_chunk, str) else ' '.join(top_chunk)}\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # --- GENERATION ---\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(Gamma_model.device)\n",
    "    outputs = Gamma_model.generate(\n",
    "        **inputs,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # --- RESPONSE CLEANING ---\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_response.replace(prompt, \"\").strip() if format_answer_text else full_response\n",
    "    \n",
    "    # --- RETURN ---\n",
    "    return answer if return_answer_only else (answer, top_chunk, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382a214b-945b-44ff-8dff-36ae484ccc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "#6. Testing Function\n",
    "# ====================\n",
    "def test_rag_system(query: str, \n",
    "                   show_context: bool = True,\n",
    "                   max_new_tokens: int = 256) -> str:\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    answer, context_chunk, score = ask(\n",
    "        query=query,\n",
    "        return_answer_only=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        task=\"qa\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nANSWER:\\n{answer}\\n\")\n",
    "    \n",
    "    if show_context:\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"CONTEXT (Score: {score:.4f}):\")\n",
    "        print(f\"{'-'*50}\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fce74c-56c6-43df-82a0-e37fcb479e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUERY: what is the net change in revenue from 2007 to 2008?\n",
      "==================================================\n",
      "\n",
      "ANSWER:\n",
      "21% decrease in net sales from 2008 to 2009.\n",
      "\n",
      "--------------------------------------------------\n",
      "CONTEXT (Score: 0.7002):\n",
      "--------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"what is the net change in revenue from 2007 to 2008?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_rag_system(query)\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9f993-2fe3-4887-99f6-3b2e24e8e87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
