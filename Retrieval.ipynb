{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a95734-b590-4bee-b9f1-a45c96553550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 1. Setup & Imports\n",
    "# ====================\n",
    "import pandas as pd\n",
    "import pinecone\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "load_dotenv()\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0089cf12-1732-4f8e-b876-d0b54cf9f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 2. Load & Preprocess Data\n",
    "# ====================\n",
    "def load_financial():\n",
    "    df = pd.read_csv(\"finaicial.csv\")\n",
    "    return df\n",
    "df = load_financial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc7c019-35fa-47b8-964b-a1825ff4d41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd4bff516</td>\n",
       "      <td>containerboard , kraft papers and saturating k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd4c55cc2</td>\n",
       "      <td>entergy mississippi , inc .\\nmanagement's fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd4c5a718</td>\n",
       "      <td>we have a five year $ 1350 million revolving ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd4be0184</td>\n",
       "      <td>the agreements that govern the indebtedness in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd4b93b5e</td>\n",
       "      <td>during 2005 , we amended our $ 1.0 billion uns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id                                               text\n",
       "0  dd4bff516  containerboard , kraft papers and saturating k...\n",
       "1  dd4c55cc2  entergy mississippi , inc .\\nmanagement's fina...\n",
       "2  dd4c5a718  we have a five year $ 1350 million revolving ,...\n",
       "3  dd4be0184  the agreements that govern the indebtedness in...\n",
       "4  dd4b93b5e  during 2005 , we amended our $ 1.0 billion uns..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38323bf-c65e-4637-acac-154485e53d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 4. Sentence Chunking\n",
    "# ====================\n",
    "def split_into_chunks(text, chunk_size=10):\n",
    "    \"\"\"Split text into chunks of `chunk_size` sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [str(sent) for sent in doc.sents]\n",
    "    return [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "# Process text and tables\n",
    "df[\"sentence_chunks\"] = df[\"text\"].apply(split_into_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77faef4-ae9f-4244-ad94-45ba4bf71563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5. Tokens Counts\n",
    "# =====================\n",
    "#token limits for all-mpnet-base-2 is 384\n",
    "def counts(text):\n",
    "    \"\"\"checking the tokens to see if its within the token limit\"\"\"\n",
    "    return len(text)\n",
    "df[\"sentence_token_counts\"] = df[\"sentence_chunks\"].apply(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43b4cae-a10c-451a-bcb5-345910c64dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_chunks</th>\n",
       "      <th>sentence_token_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd4bff516</td>\n",
       "      <td>containerboard , kraft papers and saturating k...</td>\n",
       "      <td>[[containerboard , kraft papers and saturating...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd4c55cc2</td>\n",
       "      <td>entergy mississippi , inc .\\nmanagement's fina...</td>\n",
       "      <td>[[entergy mississippi , inc .\\n, management's ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd4c5a718</td>\n",
       "      <td>we have a five year $ 1350 million revolving ,...</td>\n",
       "      <td>[[we have a five year $ 1350 million revolving...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd4be0184</td>\n",
       "      <td>the agreements that govern the indebtedness in...</td>\n",
       "      <td>[[the agreements that govern the indebtedness ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd4b93b5e</td>\n",
       "      <td>during 2005 , we amended our $ 1.0 billion uns...</td>\n",
       "      <td>[[during 2005 , we amended our $ 1.0 billion u...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         _id                                               text  \\\n",
       "0  dd4bff516  containerboard , kraft papers and saturating k...   \n",
       "1  dd4c55cc2  entergy mississippi , inc .\\nmanagement's fina...   \n",
       "2  dd4c5a718  we have a five year $ 1350 million revolving ,...   \n",
       "3  dd4be0184  the agreements that govern the indebtedness in...   \n",
       "4  dd4b93b5e  during 2005 , we amended our $ 1.0 billion uns...   \n",
       "\n",
       "                                     sentence_chunks  sentence_token_counts  \n",
       "0  [[containerboard , kraft papers and saturating...                      4  \n",
       "1  [[entergy mississippi , inc .\\n, management's ...                      2  \n",
       "2  [[we have a five year $ 1350 million revolving...                      5  \n",
       "3  [[the agreements that govern the indebtedness ...                      3  \n",
       "4  [[during 2005 , we amended our $ 1.0 billion u...                      4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8794fb-3670-43b1-962c-fa1c0ae37111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_token_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.910939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_token_counts\n",
       "count            2066.000000\n",
       "mean                2.910939\n",
       "std                 1.007385\n",
       "min                 1.000000\n",
       "25%                 2.000000\n",
       "50%                 3.000000\n",
       "75%                 3.000000\n",
       "max                10.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911a1189-8882-4117-bcf5-baa408bbe6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 6. Embedding Generation\n",
    "# ====================\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"Generate emdeddings layers\"\"\"\n",
    "    # Flatten chunks if they're nested lists\n",
    "    flat_chunks = [\" \".join(chunk) if isinstance(chunk, list) else chunk for chunk in chunks]\n",
    "    return embedding_model.encode(flat_chunks, convert_to_numpy=True)\n",
    "\n",
    "# Generate embeddings correctly\n",
    "df[\"text_embeddings\"] = df[\"sentence_chunks\"].apply(\n",
    "    lambda chunks: generate_embeddings(chunks)\n",
    ")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"Sample embeddings shape: {df['text_embeddings'].iloc[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b974ef5-5468-4a11-bcb6-761661596863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 6014, Total embeddings: 6014\n"
     ]
    }
   ],
   "source": [
    "# Flatten chunks and embeddings\n",
    "all_text_chunks = [chunk for doc_chunks in df[\"sentence_chunks\"] for chunk in doc_chunks]\n",
    "all_text_embeddings = [emb for doc_embs in df[\"text_embeddings\"] for emb in doc_embs]\n",
    "\n",
    "# Check consistency\n",
    "assert len(all_text_chunks) == len(all_text_embeddings), \"Mismatch between chunks and embeddings!\"\n",
    "print(f\"Total chunks: {len(all_text_chunks)}, Total embeddings: {len(all_text_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc23f1a-7cfe-4eac-ac43-f2a716e89eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 7. Vector Search \n",
    "# ====================\n",
    "class VectorSearch:\n",
    "    def __init__(self, embeddings, texts):\n",
    "        # Stack embeddings into (N, 768) tensor\n",
    "        self.embeddings = torch.tensor(np.stack(embeddings), dtype=torch.float32).to(device)\n",
    "        self.texts = texts\n",
    "    \n",
    "    def search(self, query, top_k=1):\n",
    "        \n",
    "        \"\"\"Search for top_k most similar chunks (now defaults to top 1)\"\"\"\n",
    "        query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(query_embedding, self.embeddings)[0]\n",
    "        top_indices = torch.topk(cos_scores, k=top_k).indices.cpu().numpy()\n",
    "        return [(self.texts[i], cos_scores[i].item()) for i in top_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20eea57-caa8-4fb6-9b60-8a6d45ee4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize search\n",
    "text_searcher = VectorSearch(all_text_embeddings, all_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbac919-3253-4217-9c05-7fd7a7c53c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results for 'what was the increase in the operating profit for space systems from 2011 to 2012?':\n",
      "\n",
      "Rank 1 (Score: 0.7500):\n",
      "['| 13.0% ( 13.0 % )\\nbacklog at year-end | $ 18900          | $ 20500          | $ 18100         \\n\\n2014 compared to 2013 space systems 2019 net sales for 2014 increased $ 107 million , or 1% ( 1 % ) , compared to 2013 .\\n', 'the increase was primarily attributable to higher net sales of approximately $ 340 million for the orion program due to increased volume ( primarily the first unmanned test flight of the orion mpcv ) ; and about $ 145 million for commercial space transportation programs due to launch-related activities .\\n', 'the increases were offset by lower net sales of approximately $ 335 million for government satellite programs due to decreased volume ( primarily aehf , gps-iii and muos ) ; and about $ 45 million for various other programs due to decreased volume .\\nspace systems 2019 operating profit for 2014 was comparable to 2013 .\\n', 'operating profit decreased by approximately $ 20 million for government satellite programs due to lower volume ( primarily aehf and gps-iii ) , partially offset by increased risk retirements ( primarily muos ) ; and about $ 20 million due to decreased equity earnings for joint ventures .\\n', 'the decreases were offset by higher operating profit of approximately $ 30 million for the orion program due to increased volume .\\noperating profit was reduced by approximately $ 40 million for charges , net of recoveries , related to the restructuring action announced in november 2013 .\\n', 'adjustments not related to volume , including net profit booking rate adjustments and other matters , were approximately $ 10 million lower for 2014 compared to 2013 .\\n', '2013 compared to 2012 space systems 2019 net sales for 2013 decreased $ 389 million , or 5% ( 5 % ) , compared to 2012 .\\n', 'the decrease was primarily attributable to lower net sales of approximately $ 305 million for commercial satellite programs due to fewer deliveries ( zero delivered during 2013 compared to two for 2012 ) ; and about $ 290 million for the orion program due to lower volume .\\n', 'the decreases were partially offset by higher net sales of approximately $ 130 million for government satellite programs due to net increased volume ; and about $ 65 million for strategic and defensive missile programs ( primarily fbm ) due to increased volume and risk retirements .\\n', 'the increase for government satellite programs was primarily attributable to higher volume on aehf and other programs , partially offset by lower volume on goes-r , muos and sbirs programs .\\nspace systems 2019 operating profit for 2013 decreased $ 38 million , or 4% ( 4 % ) , compared to 2012 .\\n']\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 8. Example Query\n",
    "# ====================\n",
    "query = \"what was the increase in the operating profit for space systems from 2011 to 2012?\"\n",
    "results = text_searcher.search(query)\n",
    "\n",
    "print(f\"Top results for '{query}':\")\n",
    "for i, (chunk, score) in enumerate(results):\n",
    "    print(f\"\\nRank {i + 1} (Score: {score:.4f}):\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b35eb89d-0037-41f2-9851-2780bdbf417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'datatonic-rags' is ready!\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Save in vector database\n",
    "# ========================\n",
    "vector_db = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = pinecone.Pinecone(api_key=\"pcsk_7B7VXN_6M4qLKUbxBrU4iCXs5VVy4ZCQCoTJUNJayD2EJa6PeqGygBfxzBb64YL2D56C9U\")\n",
    "index_name = \"datatonic-rags\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768, \n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\" \n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Index '{index_name}' is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0517501a-1037-4e76-ae11-bb4102902dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# 10. Push embeddings to Vector Database\n",
    "#==================================\n",
    "vectors_to_upsert = [\n",
    "    (\n",
    "        f\"vec_{i}\",  # Unique ID for each vector\n",
    "        emb.tolist() if hasattr(emb, 'tolist') else emb,  # Ensure it's a list\n",
    "        {\"text\": chunk}  # Store the text in metadata\n",
    "    )\n",
    "    for i, (chunk, emb) in enumerate(zip(all_text_chunks, all_text_embeddings))\n",
    "]\n",
    "\n",
    "# 2. Batch upsert (Pinecone recommends batches of 100-200)\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(vectors_to_upsert), batch_size)):\n",
    "    # Get batch of vectors\n",
    "    i_end = min(i+batch_size, len(vectors_to_upsert))\n",
    "    batch = vectors_to_upsert[i:i_end]\n",
    "    \n",
    "    # Upsert to Pinecone\n",
    "    try:\n",
    "        index.upsert(vectors=batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting batch {i}-{i_end}: {e}\")\n",
    "        # Optionally: retry or save failed batches\n",
    "\n",
    "print(\"Upsert complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "945e16d3-a64b-49c2-a47e-21d9b1d0ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeRetriever:\n",
    "    def __init__(self, index_name=\"datatonic-rags\", embedding_model=None):\n",
    "        vector_db = os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.pc = pinecone.Pinecone(api_key=vector_db)\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.embedding_model = embedding_model\n",
    "    def query(self, query: str, top_k: int = 1):\n",
    "        query_embedding = self.embedding_model.encode(query).tolist()\n",
    "        results = self.index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        chunks_with_scores = [(match.metadata[\"text\"], match.score) for match in results.matches]\n",
    "        return chunks_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64827967-a91d-40b9-9654-722109228842",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = PineconeRetriever(embedding_model = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "350ff416-d396-40cb-bca5-098849a59f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['meet customer needs and put us in a position to handle demand changes .\\n', 'we will also continue utilizing industrial engineering techniques to improve productivity .\\n', '2022 fuel prices 2013 uncertainty about the economy makes fuel price projections difficult , and we could see volatile fuel prices during the year , as they are sensitive to global and u.s .\\ndomestic demand , refining capacity , geopolitical issues and events , weather conditions and other factors .\\nto reduce the impact of fuel price on earnings , we will continue to seek recovery from our customers through our fuel surcharge programs and to expand our fuel conservation efforts .\\n', '2022 capital plan 2013 in 2010 , we plan to make total capital investments of approximately $ 2.5 billion , including expenditures for ptc , which may be revised if business conditions or new laws or regulations affect our ability to generate sufficient returns on these investments .\\n', 'see further discussion in this item 7 under liquidity and capital resources 2013 capital plan .\\n', '2022 positive train control ( ptc ) 2013 in response to a legislative mandate to implement ptc by the end of 2015 , we expect to spend approximately $ 200 million during 2010 on the development of ptc .\\n', 'we currently estimate that ptc will cost us approximately $ 1.4 billion to implement by the end of 2015 , in accordance with rules issued by the fra .\\n', 'this includes costs for installing the new system along our tracks , upgrading locomotives to work with the new system , and adding digital data communication equipment so all the parts of the system can communicate with each other .\\n', '2022 financial expectations 2013 we remain cautious about economic conditions but expect volume to increase from 2009 levels .\\n', 'in addition , we anticipate continued pricing opportunities and further productivity improvements .\\n'], 0.447262287)]\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is Tesla Stcok in year 2023\"\n",
    "relevant_chunks = retrieval.query(user_query)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba1752-d6a2-4b54-8214-810d06ada903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
